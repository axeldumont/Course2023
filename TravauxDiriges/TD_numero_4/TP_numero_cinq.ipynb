{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuvignyEnsta/Promotion_2023/blob/main/TP_numero_cinq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etdu6cUmeDmX"
      },
      "source": [
        "# Configurer votre colab pour le calcul sur GPU\n",
        "Dans le menu au dessus, choisir le sous-menu **Exécution** puis l'option **Modifier le type d'exécution**. \n",
        "\n",
        "Une fenêtre apparaît où vous pouvez choisir un accélérateur matériel. Choisissez un *GPU* puis *enregistrer* votre choix.\n",
        "\n",
        "Afin de vérifier que vous avez bien configuré votre session pour utiliser un GPU, exécuter le code ci-dessous (en passant la souris, une petite flèche pour l'exécuter apparaît).\n",
        "\n",
        "Si tout se passe bien, un tableau (en ascii) apparaît avec le type de carte (et son architecture) auquel vous avez accès. Si un message d'erreur apparaît, vérifiez dans **Modifier le type d'exécution** que l'accélérateur matériel est bien configuré pour un *GPU*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AisG7sW_bImi",
        "outputId": "5cf0dc61-e632-496e-de9e-c26012dd7e17",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKT2rlVxgil3"
      },
      "source": [
        "# Installation de PyCuda\n",
        "\n",
        "Pour utiliser le GPU sous Python il faut soit installer **PyCuda** pour effectuer des calculs, soit **PyTorch** pour effectuer du machine learning accéléré par les GPUs.\n",
        "\n",
        "Dans ce TP, nous nous bornerons uniquement à effectuer des calculs sur GPU !\n",
        "\n",
        "Pour cela, il faut d'abord installer pyCuda et donc exécuter le code ci-dessous (toujours en appuyant sur la flèche pour exécuter le code, et attendez, cela prend un petit moment...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juIs9Ostgkoc",
        "outputId": "cd105ad7-65c9-4fcf-a2a5-c70680ee10fc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "!pip install pycuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qudDlvJtlFvM"
      },
      "source": [
        "# Vérification du bon fonctionnement du module PyCuda\n",
        "\n",
        "Afin de vérifier le bon fonctionnement de l'installation, nous allons écrire un script Cuda qui servira en même temps d'exemple pour le TP d'aujourd'hui. Ce script sera scindé en plusieurs morceaux et commenté pour que vous compreniez chaque partie du code.\n",
        "\n",
        "Ce code sera très simple : on va créer une matrice 4x4 avec des données prises \"au hasard\" (mais avec une graine aléatoire fixée, donc...) , recopier ce tableau dans la mémoire de la carte graphique et demander à la carte graphique de doubler la valeur de chaque élément de la matrice puis de recopier dans la mémoire de l'ordinateur les valeurs calculées. On affiche ensuite la matrice initiale et la matrice transformée.\n",
        "\n",
        "Dans un premier temps, chargeons les modules python nécessaire à l'exécution d'un noyau Cuda (**Remarque** : l'importation de ces modules n'est nécessaire qu'une seule fois pour **TOUTE** la session. Il sera donc inutile de reimporter ces modules dans le reste du TD !) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqWKO6k_mw45",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDlqPZFGm1mH"
      },
      "source": [
        "Créons à l'aide de numpy un tableau *a* dont les valeurs aléatoires sont issues d'une graine fixée :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHjGxA72nD5Q",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "numpy.random.seed(1729)\n",
        "a = numpy.random.randn(4,4) # Sous forme de matrice 4 x 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB_UhGnJnL0H"
      },
      "source": [
        "Puisque certaines cartes graphiques supportent mal le double précision, nous allons demander à Python que les éléments de *a* soient en simple précision :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN3tt27lnZYf",
        "outputId": "7187182f-f44a-4888-c2e1-051efd8741bf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "a = a.astype(numpy.float32)\n",
        "print(a.nbytes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFvuQK_wnccf"
      },
      "source": [
        "Nous allons ensuite allouer de la place mémoire sur la mémoire vive du GPU. Comme en C, la place mémoire est exprimée en *Octets* :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WXc0_Q8nqw3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "a_gpu = cuda.mem_alloc(a.nbytes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqIHO5oYnugQ"
      },
      "source": [
        "Une fois la réservation sur le GPU de faite, on recopie les valeurs de *a* dans le tableau *a_gpu* que l'on vient de réserver (htod signifie host **to** device):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMiVGjKEoAJJ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "cuda.memcpy_htod(a_gpu, a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2EIIhLwoDLY"
      },
      "source": [
        "Il faut ensuite écrire le noyau qui s'exécutera sur la carte graphique à l'aide du langage CUDA (qui est une extension du langage C).\n",
        "\n",
        "**Remarquez** dans le code qu'on s'assure que les indices donnés par les numéros de threads ne dépassent pas la dimension de la matrice !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_uaN7JNok9-",
        "outputId": "3b301950-12c0-486d-a7f8-86052ca3d327",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "mod = SourceModule(\"\"\"\n",
        "__global__ void doublify( int dim_x, int dim_y, float *a )\n",
        "{\n",
        "  if ((threadIdx.x < dim_x) && (threadIdx.y < dim_y))\n",
        "  {\n",
        "    int idx = threadIdx.x + threadIdx.y * dim_x;\n",
        "    a[idx] *= 2;\n",
        "  }\n",
        "}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqsGbd-LqAlF"
      },
      "source": [
        "Nous allons récupérer ensuite un \"handle\" sur le noyau cuda que l'on vient de créer (en invoquant le nom de la fonction) et l'invoquer sur la carte graphique en dimensionnant la grille de calcul à l'aide de *block=(4,4,1)* (et en l'adaptant aux dimensions de la matrice).\n",
        "\n",
        "Ici, nous n'avons pas besoin de configurer de grilles car la dimension de la matrice *a* est petite, mais comme la taille d'un bloc dans une direction est limitée à 256, pour des cas plus gros, il faudra également passer une grille en paramètre à l'aide de *grid=(x,y,z)* où *x,y* et *z* sont des valeurs entières positives et utiliser dans le noyau cuda les valeurs *x,y* et *z* de **blockIdx** et **blockDim**.\n",
        "\n",
        "**REMARQUE IMPORTANTE** : Le noyau cuda ne veut que des types C basiques en argument. Or un entier python est tout sauf un type basique du C. Donc pour passer un argument entier à un noyau cuda, il faut créer un entier 32 bits à l'aide de numpy.int32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW9-l0XaqSuH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "func = mod.get_function(\"doublify\")\n",
        "dim = numpy.int32(4)\n",
        "func(dim, dim, a_gpu, block=(4,4,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbE0Dg-7qRlt"
      },
      "source": [
        "Il ne reste plus qu'à récupérer les nouvelles valeurs de la matrice en transférant les données de la matrice contenue par le GPU dans un tableau numpy se trouvant dans la RAM de l'ordinateur (dtoh signifie device **to** host) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt77Pr7JqrPU",
        "outputId": "0d1521f0-2e16-4516-c22f-b758e1bca245",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "a_doubled = numpy.empty_like(a)\n",
        "cuda.memcpy_dtoh(a_doubled, a_gpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCEHT9OMwLCl"
      },
      "source": [
        "Il ne reste plus qu'à afficher la matrice initiale puis la matrice obtenue par le calcul sur GPU :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7-qzHVFwP9R",
        "outputId": "6a35e2c4-f006-401e-edfc-1c99ad0d15aa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "print(f\"a : {a}\")\n",
        "print(f\"a_gpu : {a_doubled}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loEHbBbLwxwg"
      },
      "source": [
        "## Exercices de mises en bouche\n",
        "\n",
        "En vous inspirant du programme commenté ci-dessus, écrivez un code avec un noyau Cuda qui fait la somme de deux vecteurs de réels simples précisions et qui range le résultat dans un troisième vecteur. \n",
        "\n",
        "On s'exercera à utiliser des vecteurs de \"grandes\" dimensions (> 256) afin de s'entrainer à calculer des indices globaux dans le noyau cuda à l'aide de *threadIdx.x*, *blockIdx.x* et *dimBlock.x*.\n",
        "\n",
        "Ecrivez (ou copier coller) votre code dans la cellule ci-dessous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdP6BpWQyDyx",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esWwXhGMy8vJ"
      },
      "source": [
        "Une fois le code mis au point, recopiez le ci-dessous et adaptez le pour additionner deux matrices carrées dans une troisième."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMAqLHVmzJbG",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_67yypc0yL0g"
      },
      "source": [
        "# Exercice final\n",
        "\n",
        "Nous allons de nouveau nous intéresser à l'ensemble de Mandelbrot. La cellule ci-dessous contient un code permettant de calculer à l'aide de numpy un ensemble de mandelbrot (le code est simplifié par rapport aux codes sur CPU pour faciliter le TP même si il utilise la notation vectorielle de numpy (il faudra y faire attention pour code le noyau cuda !) : code repris de https://stackoverflow.com/questions/60467316/displaying-mandelbrot-set-in-python-using-matplotlib-pyplot-and-numpy).\n",
        "\n",
        "Transformez le de sorte que sur GPU, chaque thread calcule un pixel de l'image de destination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "YcAhzA4Gzb51",
        "outputId": "adb80f6c-9468-419e-8559-6a496ed9e055",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import pylab as plt\n",
        "import numpy as np\n",
        "# initial values \n",
        "loop = 100 # number of interations\n",
        "div = 1000 # divisions\n",
        "# all possible values of c\n",
        "c = np.linspace(-2,2,div)[:,np.newaxis] + 1j*np.linspace(-1.5,1.5,div)[np.newaxis,:]\n",
        "# array of ones of same dimensions as c\n",
        "ones = np.ones(np.shape(c), np.int)\n",
        "# Array that will hold colors for plot, initial value set here will be\n",
        "# the color of the points in the mandelbrot set, i.e. where the series\n",
        "# converges.\n",
        "# For the code below to work, this initial value must at least be 'loop'.\n",
        "# Here it is loop + 5\n",
        "color = ones * loop + 5\n",
        "z = 0\n",
        "for n in range(0,loop):\n",
        "      z = z**2 + c\n",
        "      diverged = np.abs(z)>2\n",
        "      # Store value of n at which series was detected to diverge.\n",
        "      # The later the series is detected to diverge, the higher\n",
        "      # the 'color' value.\n",
        "      color[diverged] = np.minimum(color[diverged], ones[diverged]*n)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [12, 7.5]\n",
        "# contour plot with real and imaginary parts of c as axes\n",
        "# and colored according to 'color'\n",
        "plt.contourf(c.real, c.imag, color)\n",
        "plt.xlabel(\"Real($c$)\")\n",
        "plt.ylabel(\"Imag($c$)\")\n",
        "plt.xlim(-2,2)\n",
        "plt.ylim(-1.5,1.5)\n",
        "plt.savefig(\"plot.png\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNfTREpmRZu+VfzoJQDE/VV",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
